{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook is a document that contains multiple cells of code and markdown. It demonstrates the process of training and testing a model using the PPO algorithm in the Gymnasium environment. The notebook includes the installation of necessary packages, the creation of the environment, the definition of a dummy model, the training and testing of the model, and the visualization of the results. It also includes the use of OpenCV for video recording and rendering. The notebook showcases the use of various variables, modules, and functions to perform the desired tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym version: 0.28.1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import stable_baselines3 as sb3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check versions\n",
    "print(f\"gym version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the pendulum environment built into Gymnasium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function to test the given agent (“model”) in our environment. Note that this resets the environment and runs forever until terminated or truncated comes back False. Our model is used to predict an action from a given observation (known as the policy). This action is used to take a step (with the .step() function) in the environment, which returns a new observation, reward, and whether or not the environment has terminated/truncated.\n",
    "\n",
    "If a video handle (from OpenCV) is passed in, each step is rendered and added to the video. If a message (msg) is passed in, that text will appear on the top-left of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(env: gym.Env, model, msg=None):\n",
    "\n",
    "    # Reset environment\n",
    "    obs, info = env.reset()\n",
    "    frame = env.render()\n",
    "    ep_len = 0\n",
    "    ep_rew = 0\n",
    "\n",
    "    # Run episode until complete\n",
    "    while True:\n",
    "\n",
    "        # Provide observation to policy to predict the next action\n",
    "        action, _ = model.predict(obs)\n",
    "\n",
    "        # Perform action, update total reward\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        ep_rew += reward\n",
    "\n",
    "        # Increase step counter\n",
    "        ep_len += 1\n",
    "\n",
    "        # Check to see if episode has ended\n",
    "        if terminated or truncated:\n",
    "            return ep_len, ep_rew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we create a dummy agent that simply selects actions randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model that just predicts random actions\n",
    "class DummyModel():\n",
    "\n",
    "    # Save environment\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    # Always output random action regardless of observation\n",
    "    def predict(self, obs):\n",
    "        action = self.env.action_space.sample()\n",
    "        return action, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then configure our video writer object and run a few episodes with our random agent. Feel free to download the output video (1-random.mp4) to see how this agent performs (probably poorly).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | length: 200, reward: -1401.0049560822501\n",
      "Episode 1 | length: 200, reward: -1061.0010691806383\n",
      "Episode 2 | length: 200, reward: -1412.147216340591\n",
      "Episode 3 | length: 200, reward: -974.8895139284227\n",
      "Episode 4 | length: 200, reward: -860.3235977285418\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try running a few episodes with the environment and random actions\n",
    "dummy_model = DummyModel(env)\n",
    "for ep in range(5):\n",
    "    ep_len, ep_rew = test_model(env, dummy_model, f\"Random, episode {ep}\")\n",
    "    print(f\"Episode {ep} | length: {ep_len}, reward: {ep_rew}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize our model (agent) to train with the PPO algorithm and set some hyperparameters. As noted, our hyperparameters come from the rl-baseline3-zoo repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = sb3.PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    learning_rate=0.001,       # Learning rate of neural network (default: 0.0003)\n",
    "    n_steps=1024,               # Number of steps per update (default: 2048)\n",
    "    batch_size=64,              # Minibatch size for NN update (default: 64)\n",
    "    gamma=0.9,                 # Discount factor (default: 0.99)\n",
    "    ent_coef=0.0,               # Entropy, how much to explore (default: 0.0)\n",
    "    use_sde=True,               # Use generalized State Dependent Exploration (default: False)\n",
    "    sde_sample_freq=4,          # Number of steps before sampling new noise matrix (default -1)\n",
    "    policy_kwargs={'net_arch': [64, 64]}, # 2 hidden layers, 1 output layer (default: [64, 64])\n",
    "    verbose=0                   # Print training metrics (default: 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model configured, we train. To make our demo more visually appealing, we divide the training into “rounds.” In each round, we train for a given number of steps and then test the model in our environment 100 times. The first test is recorded to video, and the episode lengths and rewards are averaged over all 100 tests.\n",
    "\n",
    "This can take 5-10 minutes. Once done, you can download the output video (2-training.mp4) to see the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing hyperparameters\n",
    "NUM_ROUNDS = 20\n",
    "NUM_TRAINING_STEPS_PER_ROUND = 5000\n",
    "NUM_TESTS_PER_ROUND = 100\n",
    "MODEL_FILENAME_BASE = \"pendulum-ppo\"\n",
    "VIDEO_FILENAME = \"2-training.mp4\"\n",
    "\n",
    "# Train and test the model for a number of rounds\n",
    "avg_ep_lens = []\n",
    "avg_ep_rews = []\n",
    "for rnd in range(NUM_ROUNDS):\n",
    "\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=NUM_TRAINING_STEPS_PER_ROUND)\n",
    "\n",
    "    # Save the model\n",
    "    model.save(f\"{MODEL_FILENAME_BASE}_{rnd}\")\n",
    "\n",
    "    # Test the model in several episodes\n",
    "    avg_ep_len = 0\n",
    "    avg_ep_rew = 0\n",
    "    for ep in range(NUM_TESTS_PER_ROUND):\n",
    "\n",
    "        # Only record the first test\n",
    "        if ep == 0:\n",
    "            ep_len, ep_rew = test_model(env, model, f\"Round {rnd}\")\n",
    "        else:\n",
    "            ep_len, ep_rew = test_model(env, model)\n",
    "\n",
    "        # Accumulate average length and reward\n",
    "        avg_ep_len += ep_len\n",
    "        avg_ep_rew += ep_rew\n",
    "\n",
    "    # Record and dieplay average episode length and reward\n",
    "    avg_ep_len /= NUM_TESTS_PER_ROUND\n",
    "    avg_ep_lens.append(avg_ep_len)\n",
    "    avg_ep_rew /= NUM_TESTS_PER_ROUND\n",
    "    avg_ep_rews.append(avg_ep_rew)\n",
    "    print(f\"Round {rnd} | average test length: {avg_ep_len}, average test reward: {avg_ep_rew}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average test episode lengths and rewards for each round\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.tight_layout(pad=4.0)\n",
    "axs[0].plot(avg_ep_lens)\n",
    "axs[0].set_ylabel(\"Average episode length\")\n",
    "axs[0].set_xlabel(\"Round\")\n",
    "axs[1].plot(avg_ep_rews)\n",
    "axs[1].set_ylabel(\"Average episode reward\")\n",
    "axs[1].set_xlabel(\"Round\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hoose a model that provided the best average rewards, load it, and call our test_model() function using that model. For example, in the plot above, we can see that the model from round 17 gave the best average test results. So, change the MODEL_FILENAME value to “pendulum-ppo_17” and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and video settings\n",
    "MODEL_FILENAME = \"pendulum-ppo_17\"\n",
    "VIDEO_FILENAME = \"3-testing.mp4\"\n",
    "\n",
    "# Load the model\n",
    "model = sb3.PPO.load(MODEL_FILENAME)\n",
    "\n",
    "# Create recorder\n",
    "video = cv2.VideoWriter(VIDEO_FILENAME, FOURCC, FPS, (width, height))\n",
    "\n",
    "# Test the model\n",
    "ep_len, ep_rew = test_model(env, model, video, MODEL_FILENAME)\n",
    "print(f\"Episode length: {ep_len}, reward: {ep_rew}\")\n",
    "\n",
    "# Close the video writer\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, don’t forget to close your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws_mlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
